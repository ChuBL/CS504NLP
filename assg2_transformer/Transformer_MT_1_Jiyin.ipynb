{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "fSaJxmSVjfu0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ChuBL/CS504NLP/blob/main/assg2_transformer/Transformer_MT_1_Jiyin.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "tjeonwIcyI4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -U torchdata\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install 'portalocker==2.8.2'"
      ],
      "metadata": {
        "id": "iS-PR8Z2LNQB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this step, you will need to restart session to avoid future dependency errors."
      ],
      "metadata": {
        "id": "HJO2Hfk8TlVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "fSaJxmSVjfu0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O9APpf-fK4oh"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ],
      "metadata": {
        "id": "ADO1uSfjK_7N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "shX0nM04MEnt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from typing import Union, Callable, Optional\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
        "        src = src + self.dropout1(src2)\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, layer, num_layers):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n",
        "        # Adding final normalization layer\n",
        "        self.final_norm = nn.LayerNorm(layer.self_attn.embed_dim)\n",
        "\n",
        "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
        "        output = src\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        # Apply final layer normalization\n",
        "        output = self.final_norm(output)\n",
        "        return output\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    __constants__ = ['norm_first']\n",
        "\n",
        "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 512, dropout: float = 0.1,\n",
        "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
        "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
        "                 bias: bool = True, device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "                                            bias=bias, **factory_kwargs)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "                                                 bias=bias, **factory_kwargs)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward, bias=bias, **factory_kwargs)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model, bias=bias, **factory_kwargs)\n",
        "\n",
        "        self.norm_first = norm_first\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n",
        "        self.norm3 =nn.LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        # Legacy string support for activation function.\n",
        "        if isinstance(activation, str):\n",
        "            self.activation = nn._get_activation_fn(activation)\n",
        "        else:\n",
        "            self.activation = activation\n",
        "\n",
        "    # self-attention block\n",
        "    def _sa_block(self, x: Tensor,\n",
        "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n",
        "        x = self.self_attn(x, x, x,\n",
        "                           attn_mask=attn_mask,\n",
        "                           key_padding_mask=key_padding_mask,\n",
        "                           need_weights=False, is_causal=is_causal)[0]\n",
        "        return self.dropout1(x)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super().__setstate__(state)\n",
        "\n",
        "    # multihead attention block\n",
        "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
        "                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n",
        "        x = self.multihead_attn(x, mem, mem,\n",
        "                                attn_mask=attn_mask,\n",
        "                                key_padding_mask=key_padding_mask,\n",
        "                                is_causal=is_causal,\n",
        "                                need_weights=False)[0]\n",
        "        return self.dropout2(x)\n",
        "\n",
        "    # feed forward block\n",
        "    def _ff_block(self, x: Tensor) -> Tensor:\n",
        "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "        return self.dropout2(x)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt: Tensor,\n",
        "        memory: Tensor,\n",
        "        tgt_mask: Optional[Tensor] = None,\n",
        "        memory_mask: Optional[Tensor] = None,\n",
        "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "        memory_key_padding_mask: Optional[Tensor] = None,\n",
        "        tgt_is_causal: bool = False,\n",
        "        memory_is_causal: bool = False,\n",
        "    ) -> Tensor:\n",
        "        x = tgt\n",
        "        if self.norm_first:\n",
        "            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n",
        "            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask, memory_is_causal)\n",
        "            x = x + self._ff_block(self.norm3(x))\n",
        "        else:\n",
        "            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n",
        "            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n",
        "            x = self.norm3(x + self._ff_block(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    __constants__ = ['norm']\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        decoder_layer: \"TransformerDecoderLayer\",\n",
        "        num_layers: int,\n",
        "        norm: Optional[nn.Module] = None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        torch._C._log_api_usage_once(f\"torch.nn.modules.{self.__class__.__name__}\")\n",
        "        self.layers = self._get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def _get_clones(self, module, N):\n",
        "        # FIXME: copy.deepcopy() is not defined on nn.module\n",
        "        return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "    def _get_seq_len(\n",
        "        self,\n",
        "        src: Tensor,\n",
        "        batch_first: bool\n",
        ") -> Optional[int]:\n",
        "\n",
        "        if src.is_nested:\n",
        "            return None\n",
        "        else:\n",
        "            src_size = src.size()\n",
        "            if len(src_size) == 2:\n",
        "                # unbatched: S, E\n",
        "                return src_size[0]\n",
        "            else:\n",
        "                # batched: B, S, E if batch_first else S, B, E\n",
        "                seq_len_pos = 1 if batch_first else 0\n",
        "                return src_size[seq_len_pos]\n",
        "\n",
        "    def _detect_is_causal_mask(\n",
        "        self,\n",
        "        mask: Optional[Tensor],\n",
        "        is_causal: Optional[bool] = None,\n",
        "        size: Optional[int] = None,\n",
        ") -> bool:\n",
        "        # Prevent type refinement\n",
        "        make_causal = (is_causal is True)\n",
        "\n",
        "        if is_causal is None and mask is not None:\n",
        "            sz = size if size is not None else mask.size(-2)\n",
        "            causal_comparison = self._generate_square_subsequent_mask(\n",
        "                sz, device=mask.device, dtype=mask.dtype)\n",
        "\n",
        "            # Do not use `torch.equal` so we handle batched masks by\n",
        "            # broadcasting the comparison.\n",
        "            if mask.size() == causal_comparison.size():\n",
        "                make_causal = bool((mask == causal_comparison).all())\n",
        "            else:\n",
        "                make_causal = False\n",
        "\n",
        "        return make_causal\n",
        "\n",
        "    def _generate_square_subsequent_mask(\n",
        "        self,\n",
        "        sz: int,\n",
        "        device: Optional[torch.device] = None,\n",
        "        dtype: Optional[torch.dtype] = None,\n",
        ") -> Tensor:\n",
        "        if device is None:\n",
        "            device = torch.device('cpu')\n",
        "        if dtype is None:\n",
        "            dtype = torch.float32\n",
        "        return torch.triu(\n",
        "            torch.full((sz, sz), float('-inf'), dtype=dtype, device=device),\n",
        "            diagonal=1,\n",
        "        )\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None, tgt_is_causal: Optional[bool] = None,\n",
        "                memory_is_causal: bool = False) -> Tensor:\n",
        "        output = tgt\n",
        "\n",
        "        seq_len = self._get_seq_len(tgt, self.layers[0].self_attn.batch_first)\n",
        "        tgt_is_causal = self._detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
        "                         memory_mask=memory_mask,\n",
        "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                         memory_key_padding_mask=memory_key_padding_mask,\n",
        "                         tgt_is_causal=tgt_is_causal,\n",
        "                         memory_is_causal=memory_is_causal)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "YJsNi3aKhq5T"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customization"
      ],
      "metadata": {
        "id": "91H2F7xWvwri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%script false --no-raise-error\n",
        "# mujirushi\n",
        "\n",
        "#@title **Customized Transformer**\n",
        "\n",
        "\n",
        "# @markdown **【Step 1.】:**<font size=\"5\">Select the normalization for encoder.</font>\n",
        "\n",
        "# encoding:utf-8\n",
        "norm4encoder = \"No Normalization\"  # @param [\"No Normalization\",\"Post-Layer Normalization\", \"Pre-Layer Normalization\"]\n",
        "\n",
        "# @markdown **【Step 2.】:** <font size=\"5\">Select the normalization for decoder.\n",
        "\n",
        "norm4decoder = \"No Normalization\"  # @param [\"No Normalization\",\"Post-Layer Normalization\", \"Pre-Layer Normalization\"]\n",
        "\n",
        "# @markdown **【Step 3.】:** <font size=\"5\">Specify keywords to halt the training, leave blank for no halt.\n",
        "# @markdown </br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "# @markdown e.g, `igloo`, `people`, `crowd`, etc.</font>\n",
        "\n",
        "halt_keyword = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        if \"Pre-Layer Normalization\" == norm4encoder:\n",
        "            src2 = self.norm1(src)\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        if \"Post-Layer Normalization\" == norm4encoder:\n",
        "            src = self.norm1(src)\n",
        "\n",
        "        if \"Pre-Layer Normalization\" == norm4encoder:\n",
        "            src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        if \"Post-Layer Normalization\" == norm4encoder:\n",
        "            src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, layer, num_layers):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
        "        output = src\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        if \"Pre-Layer Normalization\" == norm4decoder:\n",
        "            tgt2 = self.norm1(tgt)\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        if \"Post-Layer Normalization\" == norm4decoder:\n",
        "            tgt = self.norm1(tgt)\n",
        "\n",
        "        if \"Pre-Layer Normalization\" == norm4decoder:\n",
        "            tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        if \"Post-Layer Normalization\" == norm4decoder:\n",
        "            tgt = self.norm2(tgt)\n",
        "\n",
        "        if \"Pre-Layer Normalization\" == norm4decoder:\n",
        "            tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        if \"Post-Layer Normalization\" == norm4decoder:\n",
        "            tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, layer, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        output = tgt\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "oX6nYKdWJZek"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead,\n",
        "                 src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        encoder_layer = TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "        decoder_layer = TransformerDecoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers)\n",
        "\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
        "\n",
        "    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        memory = self.encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.decoder(tgt_emb, memory, tgt_mask, None, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        return self.encoder(src_emb, src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "        return self.decoder(tgt_emb, memory, tgt_mask)"
      ],
      "metadata": {
        "id": "dMw8y87jKfn3"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "1U7CMECiO2_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "# mujirushi\n",
        "\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "metadata": {
        "id": "TSml8gw6PGrj"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "P6SkUVIKPI6G"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "-T4ihXFtPNhx"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "id": "wKdkHecePPgU"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ],
      "metadata": {
        "id": "8JZMJ2cwPUEt"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "t2rwUmbQoN5q"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    translated_sentence = translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\")\n",
        "    print(translated_sentence)\n",
        "    if \"\" == halt_keyword:\n",
        "        continue\n",
        "    if halt_keyword in translated_sentence:\n",
        "        break"
      ],
      "metadata": {
        "id": "CTuFHmwBPW8Y"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ],
      "metadata": {
        "id": "kqsQwgtNSdXn"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre/Post Comparison"
      ],
      "metadata": {
        "id": "O45GaNNiw2YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-layer Normalization Training"
      ],
      "metadata": {
        "id": "a7J1nw8Mw6OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    translated_sentence = translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\")\n",
        "    print(translated_sentence)\n",
        "    if \"\" == halt_keyword:\n",
        "        continue\n",
        "    if halt_keyword in translated_sentence:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5ioZMprw9TE",
        "outputId": "a0620a91-2e31-40d0-9306-1d628b1e253f"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 4.703, Val loss: 3.646, Epoch time = 39.862s\n",
            " A group of people are standing in front of a crowd . \n",
            "Epoch: 2, Train loss: 3.385, Val loss: 2.946, Epoch time = 40.768s\n",
            " A group of people standing outside a building . \n",
            "Epoch: 3, Train loss: 2.817, Val loss: 2.618, Epoch time = 41.840s\n",
            " A group of people standing in front of a crowd of people . \n",
            "Epoch: 4, Train loss: 2.450, Val loss: 2.403, Epoch time = 42.123s\n",
            " A group of people standing in front of a brick building . \n",
            "Epoch: 5, Train loss: 2.177, Val loss: 2.299, Epoch time = 41.669s\n",
            " A group of people stand in front of a brick building . \n",
            "Epoch: 6, Train loss: 1.961, Val loss: 2.204, Epoch time = 41.621s\n",
            " A group of people are standing in front of a metal shop . \n",
            "Epoch: 7, Train loss: 1.779, Val loss: 2.164, Epoch time = 41.680s\n",
            " A group of people standing in front of a metal store . \n",
            "Epoch: 8, Train loss: 1.619, Val loss: 2.145, Epoch time = 41.920s\n",
            " A group of people stand in front of an igloo . \n",
            "Epoch: 9, Train loss: 1.482, Val loss: 2.145, Epoch time = 41.963s\n",
            " A group of people are standing in front of an igloo . \n",
            "Epoch: 10, Train loss: 1.359, Val loss: 2.133, Epoch time = 42.060s\n",
            " A group of people stand in front of an igloo . \n",
            "Epoch: 11, Train loss: 1.249, Val loss: 2.161, Epoch time = 41.598s\n",
            " A group of people stand in front of an igloo . \n",
            "Epoch: 12, Train loss: 1.149, Val loss: 2.166, Epoch time = 41.457s\n",
            " A group of people standing in front of an igloo . \n",
            "Epoch: 13, Train loss: 1.059, Val loss: 2.215, Epoch time = 41.333s\n",
            " A group of people standing in front of an igloo . \n",
            "Epoch: 14, Train loss: 0.982, Val loss: 2.225, Epoch time = 41.486s\n",
            " A group of people standing in front of an igloo . \n",
            "Epoch: 15, Train loss: 0.920, Val loss: 2.255, Epoch time = 41.497s\n",
            " A group of people stand in front of an igloo \n",
            "Epoch: 16, Train loss: 0.859, Val loss: 2.280, Epoch time = 41.428s\n",
            " A group of people are standing in front of an igloo . \n",
            "Epoch: 17, Train loss: 0.806, Val loss: 2.264, Epoch time = 41.355s\n",
            " A group of people are standing in front of an igloo . \n",
            "Epoch: 18, Train loss: 0.757, Val loss: 2.300, Epoch time = 41.318s\n",
            " A group of people in front of an igloo . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-layer Normalization Output Log"
      ],
      "metadata": {
        "id": "S-rlM29_zqmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: 1, Train loss: 4.703, Val loss: 3.646, Epoch time = 39.862s\n",
        " A group of people are standing in front of a crowd .\n",
        "\n",
        "Epoch: 2, Train loss: 3.385, Val loss: 2.946, Epoch time = 40.768s\n",
        " A group of people standing outside a building .\n",
        "\n",
        "Epoch: 3, Train loss: 2.817, Val loss: 2.618, Epoch time = 41.840s\n",
        " A group of people standing in front of a crowd of people .\n",
        "\n",
        "Epoch: 4, Train loss: 2.450, Val loss: 2.403, Epoch time = 42.123s\n",
        " A group of people standing in front of a brick building .\n",
        "\n",
        "Epoch: 5, Train loss: 2.177, Val loss: 2.299, Epoch time = 41.669s\n",
        " A group of people stand in front of a brick building .\n",
        "\n",
        "Epoch: 6, Train loss: 1.961, Val loss: 2.204, Epoch time = 41.621s\n",
        " A group of people are standing in front of a metal shop .\n",
        "\n",
        "Epoch: 7, Train loss: 1.779, Val loss: 2.164, Epoch time = 41.680s\n",
        " A group of people standing in front of a metal store .\n",
        "\n",
        "Epoch: 8, Train loss: 1.619, Val loss: 2.145, Epoch time = 41.920s\n",
        " A group of people stand in front of an igloo .\n",
        "\n",
        "Epoch: 9, Train loss: 1.482, Val loss: 2.145, Epoch time = 41.963s\n",
        " A group of people are standing in front of an igloo .\n",
        "\n",
        "Epoch: 10, Train loss: 1.359, Val loss: 2.133, Epoch time = 42.060s\n",
        " A group of people stand in front of an igloo .\n",
        "\n",
        "Epoch: 11, Train loss: 1.249, Val loss: 2.161, Epoch time = 41.598s\n",
        " A group of people stand in front of an igloo .\n",
        "\n",
        "Epoch: 12, Train loss: 1.149, Val loss: 2.166, Epoch time = 41.457s\n",
        " A group of people standing in front of an igloo .\n",
        "\n",
        "Epoch: 13, Train loss: 1.059, Val loss: 2.215, Epoch time = 41.333s\n",
        " A group of people standing in front of an igloo .\n",
        "\n",
        "Epoch: 14, Train loss: 0.982, Val loss: 2.225, Epoch time = 41.486s\n",
        " A group of people standing in front of an igloo .\n",
        "\n",
        "Epoch: 15, Train loss: 0.920, Val loss: 2.255, Epoch time = 41.497s\n",
        " A group of people stand in front of an igloo\n",
        "\n",
        "Epoch: 16, Train loss: 0.859, Val loss: 2.280, Epoch time = 41.428s\n",
        " A group of people are standing in front of an igloo .\n",
        "\n",
        "Epoch: 17, Train loss: 0.806, Val loss: 2.264, Epoch time = 41.355s\n",
        " A group of people are standing in front of an igloo .\n",
        "\n",
        "Epoch: 18, Train loss: 0.757, Val loss: 2.300, Epoch time = 41.318s\n",
        " A group of people in front of an igloo .\n"
      ],
      "metadata": {
        "id": "HoHgzoGdz9Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-layer Normalization Training"
      ],
      "metadata": {
        "id": "4EvBPgEPxCNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    translated_sentence = translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\")\n",
        "    print(translated_sentence)\n",
        "    if \"\" == halt_keyword:\n",
        "        continue\n",
        "    if halt_keyword in translated_sentence:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF4muu7kxEm4",
        "outputId": "3c140d71-fa3a-48c2-f2a1-3476ca229352"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.324, Val loss: 4.089, Epoch time = 42.245s\n",
            " A group of people are are in a street . \n",
            "Epoch: 2, Train loss: 3.758, Val loss: 3.318, Epoch time = 41.634s\n",
            " A group of people are standing in front of a crowd . \n",
            "Epoch: 3, Train loss: 3.163, Val loss: 2.901, Epoch time = 41.893s\n",
            " A group of people standing in front of a building . \n",
            "Epoch: 4, Train loss: 2.770, Val loss: 2.633, Epoch time = 41.687s\n",
            " A group of people standing in front of a crowd . \n",
            "Epoch: 5, Train loss: 2.483, Val loss: 2.457, Epoch time = 41.822s\n",
            " A group of people standing in front of a lake . \n",
            "Epoch: 6, Train loss: 2.251, Val loss: 2.312, Epoch time = 41.692s\n",
            " A group of people standing in front of a body of water . \n",
            "Epoch: 7, Train loss: 2.057, Val loss: 2.204, Epoch time = 41.883s\n",
            " A group of people standing in front of an outdoor area . \n",
            "Epoch: 8, Train loss: 1.891, Val loss: 2.112, Epoch time = 41.835s\n",
            " A group of people stand in front of an empty shop . \n",
            "Epoch: 9, Train loss: 1.755, Val loss: 2.054, Epoch time = 42.143s\n",
            " A group of people stand in front of an empty waterfall . \n",
            "Epoch: 10, Train loss: 1.629, Val loss: 1.983, Epoch time = 41.728s\n",
            " A group of people standing in front of an empty waterfall . \n",
            "Epoch: 11, Train loss: 1.515, Val loss: 1.948, Epoch time = 41.879s\n",
            " A group of people stand in front of an empty waterfall . \n",
            "Epoch: 12, Train loss: 1.416, Val loss: 1.954, Epoch time = 41.750s\n",
            " A group of people standing in front of an empty waterfall . \n",
            "Epoch: 13, Train loss: 1.331, Val loss: 1.980, Epoch time = 41.747s\n",
            " A group of people stand in front of an operation . \n",
            "Epoch: 14, Train loss: 1.250, Val loss: 1.953, Epoch time = 41.811s\n",
            " A group of people stand in front of an axe . \n",
            "Epoch: 15, Train loss: 1.169, Val loss: 1.901, Epoch time = 41.730s\n",
            " A group of people standing in front of an igloo . \n",
            "Epoch: 16, Train loss: 1.101, Val loss: 1.902, Epoch time = 41.788s\n",
            " A group of people standing in front of an igloo . \n",
            "Epoch: 17, Train loss: 1.035, Val loss: 1.923, Epoch time = 41.864s\n",
            " A group of people standing in front of an igloo \n",
            "Epoch: 18, Train loss: 0.974, Val loss: 1.913, Epoch time = 41.696s\n",
            " A group of people stand in front of an igloo . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-layer Normalization Output Log"
      ],
      "metadata": {
        "id": "BemZbQdXz-X5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: 1, Train loss: 5.324, Val loss: 4.089, Epoch time = 42.245s\n",
        " A group of people are are in a street .\n",
        "\n",
        "Epoch: 2, Train loss: 3.758, Val loss: 3.318, Epoch time = 41.634s\n",
        " A group of people are standing in front of a crowd .\n",
        "\n",
        "Epoch: 3, Train loss: 3.163, Val loss: 2.901, Epoch time = 41.893s\n",
        " A group of people standing in front of a building .\n",
        "\n",
        "Epoch: 4, Train loss: 2.770, Val loss: 2.633, Epoch time = 41.687s\n",
        " A group of people standing in front of a crowd .\n",
        "\n",
        "Epoch: 5, Train loss: 2.483, Val loss: 2.457, Epoch time = 41.822s\n",
        " A group of people standing in front of a lake .\n",
        "\n",
        "Epoch: 6, Train loss: 2.251, Val loss: 2.312, Epoch time = 41.692s\n",
        " A group of people standing in front of a body of water .\n",
        "\n",
        "Epoch: 7, Train loss: 2.057, Val loss: 2.204, Epoch time = 41.883s\n",
        " A group of people standing in front of an outdoor area .\n",
        "\n",
        "Epoch: 8, Train loss: 1.891, Val loss: 2.112, Epoch time = 41.835s\n",
        " A group of people stand in front of an empty shop .\n",
        "\n",
        "Epoch: 9, Train loss: 1.755, Val loss: 2.054, Epoch time = 42.143s\n",
        " A group of people stand in front of an empty waterfall .\n",
        "\n",
        "Epoch: 10, Train loss: 1.629, Val loss: 1.983, Epoch time = 41.728s\n",
        " A group of people standing in front of an empty waterfall .\n",
        "\n",
        "Epoch: 11, Train loss: 1.515, Val loss: 1.948, Epoch time = 41.879s\n",
        " A group of people stand in front of an empty waterfall .\n",
        "\n",
        "Epoch: 12, Train loss: 1.416, Val loss: 1.954, Epoch time = 41.750s\n",
        " A group of people standing in front of an empty waterfall .\n",
        "\n",
        "Epoch: 13, Train loss: 1.331, Val loss: 1.980, Epoch time = 41.747s\n",
        " A group of people stand in front of an operation .\n",
        "\n",
        "Epoch: 14, Train loss: 1.250, Val loss: 1.953, Epoch time = 41.811s\n",
        " A group of people stand in front of an axe .\n",
        "\n",
        "Epoch: 15, Train loss: 1.169, Val loss: 1.901, Epoch time = 41.730s\n",
        " A group of people standing in front of an igloo .\n",
        "\n",
        "Epoch: 16, Train loss: 1.101, Val loss: 1.902, Epoch time = 41.788s\n",
        " A group of people standing in front of an igloo .\n",
        "\n",
        "Epoch: 17, Train loss: 1.035, Val loss: 1.923, Epoch time = 41.864s\n",
        " A group of people standing in front of an igloo\n",
        "\n",
        "Epoch: 18, Train loss: 0.974, Val loss: 1.913, Epoch time = 41.696s\n",
        " A group of people stand in front of an igloo .\n"
      ],
      "metadata": {
        "id": "D5RkpqXg0Aiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "d9LyWiLI0B-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pre-layer normalization outperformed the post one in the provided translation task."
      ],
      "metadata": {
        "id": "4GS-SzVc0Dsc"
      }
    }
  ]
}